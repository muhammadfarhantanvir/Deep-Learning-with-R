library(keras)
library(EBImage)

# MNIST data
mnist <- dataset_mnist()
trainx <- mnist$train$x
trainy <- mnist$train$y
testx <- mnist$test$x
testy <- mnist$test$y
table(mnist$train$y, mnist$train$y)
table(mnist$test$y, mnist$test$y)

# Plot images
par(mfrow = c(8,8), mar = rep(0, 4))
for (i in 1:64) plot(as.raster(trainx[i,,], max = 255))
par(mfrow = c(1,1))
trainx[3,,]
hist(trainx[1,,])

# Five 
a <- c(1, 12, 36, 48, 66, 101, 133, 139, 146)
par(mfrow = c(3,3), mar = rep(0, 4))
for (i in a) plot(as.raster(trainx[i,,], max = 255))
par(mfrow= c(1,1))

# Reshape & rescale
trainx <- array_reshape(trainx, c(nrow(trainx), 28, 28, 1))
testx <- array_reshape(testx, c(nrow(testx), 28, 28, 1))
trainx <- trainx / 255
testx <- testx / 255
str(trainx)
hist(trainx[1,])
trainx[1,,,]

# One hot encoding
trainy <- to_categorical(trainy, 10)
testy <- to_categorical(testy, 10)
head(trainy)

# Model
model <- keras_model_sequential()
model %>%  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', 
                       input_shape = c(28,28,1)) %>%   
         layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %>%  
         layer_max_pooling_2d(pool_size = c(2,2)) %>% 
         layer_dropout(rate = 0.25) %>%   
         layer_flatten() %>% 
         layer_dense(units = 256, activation = 'relu') %>%  
         layer_dropout(rate = 0.25) %>% 
         layer_dense(units = 10, activation = 'softmax') 

# Compile
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_adadelta(),
                  metrics = 'accuracy')

# Fit model
history <- model %>% 
         fit(trainx,
             trainy,
             epochs = 15,
             batch_size = 128,
             validation_split = 0.2)

# Evaluation & Prediction - train data 
model %>% evaluate(trainx, trainy)
pred <- model %>%   predict_classes(trainx)
table(Predicted=pred, Actual=mnist$train$y)
prob <- model %>%   predict_proba(trainx) 
cbind(prob, Predicted_class = pred, Actual = mnist$train$y)[1:5,]

# Evaluation and Prediction - Test data
model %>% evaluate(testx, testy)
pred <- model %>% predict_classes(testx)
table(Predicted = pred, Actual = mnist$test$y)
prob <- model %>% predict_proba(testx)
cbind(prob, Predicted_class = pred, Actual = mnist$test$y)[1:5,]

# New data
setwd("~/Desktop/numbers")
temp = list.files(pattern = "*.jpg")
mypic <- list()
for (i in 1:length(temp)) {mypic[[i]] <- readImage(temp[[i]])}
par(mfrow = c(3,2), mar = rep(0, 4))
for (i in 1:length(temp)) plot(mypic[[i]])
par(mfrow = c(1,1))
for (i in 1:length(temp))  {mypic[[i]] <- channel(mypic[[i]], "gray")}
for (i in 1:length(temp)) {mypic[[i]] <- 1-mypic[[i]]}
for (i in 1:length(temp)) {mypic[[i]] <- resize(mypic[[i]], 28, 28)}
mypic[[1]]
for (i in 1:length(temp)) {mypic[[i]] <- array_reshape(mypic[[i]], c(1,28,28,1))}
new <- NULL
for (i in 1:length(temp)) {new <- rbind(new, mypic[[i]])}
newx <- array_reshape(new, c(6,28,28,1))
round(newx[1,,,1], 2)
newx[1,,,1] <- ifelse(newx[1,,,1] <0.1, 0,1)
newy <- c(7, 5, 2, 0, 5, 3)

# Prediction
pred <- model %>% predict_classes(newx)
table(Predicted = pred, Actual = newy)
prob <- model %>% predict_proba(newx)
cbind(prob, Predicted = pred, Actual = newy)
